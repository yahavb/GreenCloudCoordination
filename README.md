# GreenCloudCoordination
\section{Evaluation}
%formalization of goals, refine the important construct - we have two types of inputes to the coordination system, supply and demand. explain how it will look like. Also, explain the structure of suggested solution and explain a little about it. 
In the following section we evaluate a coordination component that harmonizes \textit{analytics} jobs demands with available compute resources generated by green energy resources. Such resources will be published to the coordination system through a resource availability tuple \textit{\{region,cut-in,rated,cut-off,power-efficiency\}} where \textit{region} and \textit{power-efficiency} indicates solar-based energy and \textit{region, cut-in, rated,} and \textit{cut-off} indicates wind-based energy. 

\begin{figure}[h]
\includegraphics[width=3.0in]{DataCenterHybridPowerSource.eps}
\caption{\textbf{Hybrid Data Center Power Supply - The green clusters runs analytical jobs only when renewable energy is available while brown clusters runs serving systems. In case of unexpected drop in green energy and SLA requires, a live migration job transfers the workload into the brown clusters.}}     
\label{hybrid_dc}
\end{figure} 

\textit{Analytics} job demand includes the \textit{\{region,total-job-workload, load-factor,deadline\}} tuple. The \textit{load-factor} indicates the required number of CPU cores per the \textit{total-job-workload}. The region indication will optimize the match between the supply and demand. Also, the \textit{total-job-workload} and the \textit{deadline} will be checked against the \textit{cut-in-rated, cut-off} time for wind or \textit{power-efficiency} for solar, based on the published \textit{load-factor}. 

We will suggest a hybrid data center structure that does not deviate from the common data-center architecture. The core difference lays on automatic transfer switch (ATS) that switches between different power sources: generator, grid or clean-energy when available. In both cases the data-center design does not change and requires incremental changes only. i.e. adding clean-energy power source to the data-center's ATSs(Fig. \ref{hybrid_dc}). 

Fig. \ref{hybrid_dc} shows a simplified data-center power distribution that supports clean energy sources. In a data-center with available clean-energy resources \textit{serving} and \textit{analytical} systems deployed in brown clusters. Further, few clusters use green resources when there is a viable clean energy and standby for incoming \textit{analytical} workloads. As mitigation strategy, a compute live migration procedure will be available in case of unpredicted lack of renewable resources during a workload processing with a risk for SLO violation.

\subsection{Experiment Planning}
%What are the experiment material? which tasks have to be performed by the subjects?
We wish to simulate isolated group of compute resources so it can operate by various energy resources. We use a group of leased resources from existing cloud providers to form a virtual-data-centers set that operates in federated scheme. Each virtual-data-center includes with internal arbitrator component that collects and aggregates internal signals about its utilization and availability. Then it reports to the central coordination system. We use Apache Mesos\footnote{http://mesos.apache.org} for the virtual-data-center abstraction. 

Also, we build a highly available coordination component that accepts incoming supply and demand traffic, calculates a potential match, within minutes, and notifies back the cloud-service-provider and the service-provider for transaction completion. We use VoltDB\footnote{http://voltdb.com} as the database and application server for the coordinator component. 

Finally, we simulate customer demand for compute resources through client simulator with Java-based application that generates pseudo demand traffic to the coordinator service. The coordinator service will run on separate resources pool than the virtual-data-centers and the client simulator.(Fig. \ref{exper_arch}) 

\begin{figure}[h]
\includegraphics[width=3.8in]{exper_arch.eps}
\caption{\textbf{Experiment Architecture - Three virtual clusters deployed on a public cloud services located in a different regions grouped by Apache Mesos. A single VoltDB instance as the coordinator service. Finally, Client simulator that generates pseudo demand that sends data to process}}     
\label{exper_arch}
\end{figure} 

%Hypotheses - what are the constructs and their operationalization? they have to be traceable derived from the research question respectively the goal of the experiment. 
The data collected includes customer workload for processing, and jobs deadline. Also, the number of matches the coordination component found and processes by green clusters without SLO. Finally, we measure the false-positive cases where a match was suggested but not met the SLO's deadline due to violation that caused by a sudden lack of clean energy resources. We use the data to extrapolate the possible carbon-footprint that could be generated by the used virtual clusters. 

The research goal is to show a significant improvement in the carbon emission generation by data-centers. Let $MtCO_{\text{2}}e$ denote the carbon emission. Electrical usage can be consumed by the $Consumers$ set: 
\{cooling,storage,servers,CPU,power\} systems and denoted by $EU$ measured in kWh. The average regional carbon dioxide emissions measured in lbs/kWh and denoted by $RE_{co_{2}}$. Therefore, the total electrical usage is: 
\begin{equation}
EU_{Total}(kWh)=\sum\nolimits_{c_i \in Consumers} EU(c_{i})
\label{electric_usage}
\end{equation}
and the carbon footprint generated by the workload is:
\begin{equation}
MtCO_{\text{2}}e=\frac{EU_{Total}(kWh) \cdot RE_{co_{2}}(lbs/kWh)}{2,204.6 (lbs)}
\label{carbon_footprint}
\end{equation}
As the experiment uses virtual data-centers, we do not have access to the power consumption by the cooling, power and storage system. The results capture compute jobs durations measure in server $\frac{CPU core}{time}$ and use the average Thermal Design Power (TDP) of 200W per core (0.2kW). $c_i$ and $t_i$ denotes the number of cores and time used per job respectively. The electric usage by server ($EU_{servers}$) will be the sum of the electric utilization of the executed jobs that was matched by the coordinator component (Eq. \ref{server_usage})
\begin{equation}
EU_{servers}(kWh)=\sum_{i=1}^{jobs} c_{i} \cdot 0.2 \cdot t_{i}
\label{server_usage}
\end{equation}
We assume a linear relation between the power utilization of the cooling, power and storage systems with the servers power utilization. (Eq.\ref{electric_usage})   
\subsection{Execution}
\textbf{The Preparation.} for the execution included the setup of the three virtual data-centers each with 6 machines D series with 8 Intel(R) Xeon(R) CPU E5-2660 0 @ 2.20GHz cores, 10Gbps NICs, 28 GB memory and run Ubuntu 15.0. Each virtual data-center is located in a different geo-location. The virtual data-centers were simulated by Mesosphere cluster with three jobtracker(Mesos masters) and three tasktracker(Mesos slaves). We installed Hadoop Cloudera CDH 4.2.1-MR1 on the takstrackers. The Coordinator Database run on a separate resource pool with 1 machine D series similar to the virtual data-centers specification. Finally, the loaders run on an 3 B series machines with ADD THE SPEC OF B SERIES. 
Apache Hadoop ships with a pre-built sample app, the ubiquitous WordCount example. The input data file was created using $/dev/urandom$ on the takstrackers hosts\footnote{https://github.com/yahavb/GreenCloudCoordination/}. The input data file was copied to the HDFS directory that was created as part of the Hadoop preparation (/user/foo/data).  
%deviations - describe any deviations from the plan, e.g., how was the data collection actually performed.?
\\\textit{Deviations from the plan}. The original plan was to simulate the scenario where the customer keep his data at different location than it might be processed. We plan on using a job migration scheme that was originally designed for workload migration across different geo-location\cite{moving_jobs}. The suggested method optimizes the bandwidth costs of transferring application state and data over the wide-area network. Our experiment generated the data file at the loader host and did not include the job migration. We believe that including the job migration aspect could impact the presented results. However, the job-migration's proven efficiency and later studies minimizes that deviation.   
\\\textbf{Baseline}. The execution baseline included a load that run without the coordinator component i.e. loaders generated load to the virtual data-centers resources for 48 hours. 
The load scenario included a single file generation that was submitted to one of the tasktrackers. The output of each executed jobs included the CPU time spent for each execution. 
The data collection included the execution log of the command: hadoop jar NEED TO ADD TH RIGHT.jar wordcount /user/foo/data /user/foo/out.  

\textbf{Workloads}. The workload comprises of data files with words that needs to be counted using the Hadoop WordCount. The load complexity depends on two factors, the file randomness level and its size. We rely on the native operating system randomness and our virtual data-centers and loaders are homogeneous. Therefore, the size is the remaining factor for differentiating workload types. We evaluated the federated-cloud coordinator by generating three load types simultaneously. The three types intents to cover the following cases (1) A match was found between workload and sufficient green energy resources. (2) A match was found but there was not enough power to complete the job with no SLO deadline violation. (3) Like the former but with SLO deadline violation. The three types will be uniquely distributed across wind and solar based virtual data-centers. 

\textbf{Main Execution}. In each jobtracker host in a virtual data-center we executed a simulator that generated green availability traffic to the coordination component. The simulation comprises of availability indication that are based on Fig.\ref{rayleigh_300} and Fig.\ref{solar}. We randomized solar production by using a miss factor of $\alpha=0.2$ based on collected data between 2008-2011 in Palm Springs CA, Prescott Airport CPV, AZ and Nevada Solar One, NV\cite{NREL}. Also, we randomized the wind production by a miss factor of $\beta=0.4$ based on collected data between 2007-2012 \cite{NREL}.
